{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project analyzed data from health inspections and yelp reviews from North Carolina. The main goal is to find any correlations and patterns in the data set that can be insightful for the health inspection process as well as to help the community prevent food poisoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name & GitHub\n",
    "\n",
    "- Name: Alan Madrigal\n",
    "- GitHub Username: alanmc979"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Are families in the state of North Carolina with lower income more exposed to lower quality restaurants than those with higher income, and if so, can Yelp's ratings be use as a reliable source to go to the safest restaurants(the restaurants with the highest health inspection score and less total critical violations) in North Carolina?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First Reference: I found a similar work to what I am looking for in the city of Chicago. They showed how they predict food inspection violations which are similar to what I want to accomplish for the city of San Diego. Their method found out that they were able to improve the inspection process by finding faster the places that had food policy violations. By finding violations faster they help reduce food poisoning cases in that area. \n",
    "\n",
    "Second Reference: In the second reference, I found how the city of San Diego makes inspects restaurants. \n",
    "- 1. Each violation of the Food Inspection Report is assigned a point value depending on its importance. For example, a Major Risk Factor is worth four points, a Minor Risk Factor is worth two, and a Good Retail Practice is worth one.\n",
    "- 2. Once the Specialist completes an inspection, the points are added up and subtracted from 100. The resulting number is the inspection \"score\".\n",
    "- 3. A letter grade is assigned to the facility based on the inspection score. An \"A\" grade means the facility earned a score of 90 to 100 percent and is in satisfactory compliance with state law; a \"B\" means the facility earned a score of 80 to 89 percent and needs improvement; a \"C\" means the facility earned a score of 79 percent or less and is a failing grade.\n",
    "- 4. The grade card must be displayed near the public entrance during hours of operation. This information allows me to understand how data can be used. This information about the scoring of every place can be helpful for the prediction algorithm. \n",
    "\n",
    "Third Reference: \n",
    "    Food-borne illness affects an estimated 48 million Americans each year, resulting in 3,000 deaths and the hospitalization of 128,000 people, according to the Centers for Disease Control and Prevention (CDC). Beyond that, food-borne illness in the U.S. is enormously costly, with an estimated collective annual bill of $55 billion in medical treatment, lost productivity, and lost wages, not to mention litigation expenses. This article makes an emphasis on the impact of food poisoning and how important it is to prevent it. It also takes into consideration how inspection biases can hurt the inspection process. Hence we found that answering this data science question can help prevent food poisoning and improve the inspection process in San Diego. But we should focus on having accurate data from the inspectors without biases.\n",
    "\n",
    "References (include links):\n",
    "- 1) https://chicago.github.io/food-inspections-evaluation/\n",
    "- 2)https://www.sandiegocounty.gov/content/sdc/deh/fhd/ffis/intro.html.html\n",
    "- 3)https://hbr.org/2019/05/to-improve-food-inspections-change-the-way-theyre-scheduled\n",
    "- 4)https://www.forsyth.cc/PublicHealth/EnvironmentalHealth/aboutInspections.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that Yelp's ratings are a reliable source for selecting the restaurants that have less critical violations and better health inspection scores. I also believe that there must be a correlation between family income in an area and the quality of the food (health inspection score) in the area as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The ideal data set will have the following variables: Name (of the restaurant), Type of food (the restaurant serves), Food Inspection Grade (0-100) Number of previous year violations The data will be stored in multiple spreadsheets, each sheet for each year, which should have as many observations as the number of restaurants in San Diego. This can be stored in a CSV for easy manipulation with the Pandas module. By having all this information we can determine the relationship between the type of food and food policy violations, as well as to set a queue, with this information, for the order of the next restaurantsâ€™ inspection to find food policy violations faster.\n",
    "\n",
    "*Fill in your dataset information here*\n",
    "\n",
    "(Copy this information for each dataset)\n",
    "- Dataset Name:\n",
    "- Link to the dataset:\n",
    "- Number of observations:\n",
    "\n",
    "1-2 sentences describing each dataset. \n",
    "\n",
    "If you plan to use multiple datasets, add 1-2 sentences about how you plan to combine these datasets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Field\n",
    "\n",
    "Description\n",
    "\n",
    "HSISID\n",
    "\n",
    "State code identifying the restaurant (also the primary key to identify the restaurant)\n",
    "\n",
    "Name\n",
    "\n",
    "Name of the restaurant\n",
    "\n",
    "Address1\n",
    "\n",
    "Street address portion of the physical address of the restaurant\n",
    "\n",
    "Address2\n",
    "\n",
    "Street address portion of the physical address of the restaurant\n",
    "\n",
    "City\n",
    "\n",
    "City portion of the physical address of the restaurant\n",
    "\n",
    "State\n",
    "\n",
    "State portion of the physical address of the restaurant\n",
    "\n",
    "PostalCode\n",
    "\n",
    "Postal Code (Zip Code) portion of the physical address of the restaurant\n",
    "\n",
    "PhoneNumber\n",
    "\n",
    "Phone number of the restaurant\n",
    "\n",
    "RestaurantOpenDate\n",
    "\n",
    "Date that the restaurant opened for business\n",
    "\n",
    "FacilityType\n",
    "\n",
    "Restaurant, Food Stand, Mobile Food Unit, etc.\n",
    "\n",
    "X\n",
    "\n",
    "Coordinates of the restaurant location (X)\n",
    "\n",
    "Y\n",
    "\n",
    "Coordinates of the restaurant location (Y)\n",
    "\n",
    "GeocodeStatus\n",
    "\n",
    "Describes whether this particular record was geocoded, meaning the address that is listed was able to be matched\n",
    "\n",
    "to coordinates. M = Matched, U = Unmatched, T = Tied\n",
    "\n",
    "PermitID\n",
    "\n",
    "The permit issued for this facility\n",
    "\n",
    "Restaurants Metadata:\n",
    "\n",
    "| Field | Description|\n",
    "| --- | --- |\n",
    "| HSISID | .8 |\n",
    "| Name | .8 |\n",
    "| Address1 | .8 |\n",
    "| Address2 | .8 |\n",
    "| City | .8 |\n",
    "| State | .8 |\n",
    "| Postal/Code | .8 |\n",
    "| PhoneNumber | .8 |\n",
    "| RestaurantOpenDate | .8 |\n",
    "| FacilityType | .8 |\n",
    "| X | .8 |\n",
    "| Y | .8 |\n",
    "| GeocodeStatus | .8 |\n",
    "| PermitID | .8 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns, numpy as np\n",
    "import patsy\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Set\n",
    "df_insp = pd.read_csv('./data/inspections.csv')\n",
    "df_yelp = pd.read_csv('./data/yelp.csv')\n",
    "df_zip = pd.read_csv('./data/zipcodes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your data cleaning steps here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check out data \n",
    "df_insp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restaurants\n",
    "df_restaurants.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_rest = len(df_restaurants.name.unique())\n",
    "num_of_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of restaurants in inspection data set\n",
    "len(df_insp.hsisid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have multiple inspections for one restaurant\n",
    "#Lets Clean the Data frame by removing unecessary columns such as phone\n",
    "df1 = df_insp.filter(['hsisid', 'date','postalcode', 'name', 'address1','x','y', 'city','score','num_critical_previous', 'num_non_critical_previous', 'num_non_critical','num_critical','critical'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_zipcode(string):\n",
    "    string = str(string)\n",
    "    #Get only first five digits\n",
    "    return int(string[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['postalcode'] = df1['postalcode'].apply(standardize_zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Date and sort it\n",
    "df1[\"date\"] = pd.to_datetime(df1[\"date\"])\n",
    "df1 = df1.sort_values(by=\"date\",ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for all duplicate to check if we are able to eliminate duplicates hsisid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the above we have multiple inspections for the same restaurant through a period of time since they open.\n",
    "#Lets store the latest inspection only since we want to see the latest inspections scores and used those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dups = df1\n",
    "df1 = df1.drop_duplicates(subset=['hsisid'], keep='last')\n",
    "df1 = df1.reset_index()\n",
    "#Add column total inspections and set as 1\n",
    "print(df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we need to store the number of inspections and total number of \n",
    "total_insp_lst = []\n",
    "for hid in df1['hsisid']:\n",
    "    #get total number of inspections done\n",
    "    total_insp = len(df_with_dups[df_with_dups['hsisid'] == hid ])\n",
    "    total_insp_lst.append(total_insp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['total_num_inspections'] = total_insp_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get total critical from previous inspections hence add all num_critical_previous and num_critical\n",
    "total_critical = []\n",
    "for hid in df1['hsisid']:\n",
    "    #get total number of inspections done\n",
    "    df_cr = df_with_dups[df_with_dups['hsisid'] == hid ]\n",
    "    total = df_cr['num_critical_previous'].sum()\n",
    "    total_critical.append(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['total_critical'] = total_critical\n",
    "df1['total_critical'] += df1['num_critical']\n",
    "#Remove previous since it doesn't indicate the right number of previous critical violations\n",
    "df1 = df1.drop(columns=['num_critical_previous'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get total non_critical from previous inspections hence add all num_non_critical_previous and num_non_critical\n",
    "total_non_critical = []\n",
    "for hid in df1['hsisid']:\n",
    "    #get total number of non_critical done\n",
    "    df_cr = df_with_dups[df_with_dups['hsisid'] == hid ]\n",
    "    total = df_cr['num_non_critical_previous'].sum()\n",
    "    total_non_critical.append(total)\n",
    "len(total_non_critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['total_non_critical'] = total_non_critical\n",
    "df1['total_non_critical'] += df1['num_non_critical']\n",
    "df1 = df1.drop(columns=['num_non_critical_previous'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From restaurants get facility type\n",
    "df1 = pd.merge(df1, df_restaurants.filter(['hsisid','facilitytype']),  on='hsisid', how='inner')\n",
    "#Remove any row that is not a restaurant\n",
    "print(df1.shape)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 now stores only restaurants scores, total number of inspections (critical/non critical),\n",
    "#and last score, as well as address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Yelp data to join with df\n",
    "print(df_yelp.shape)\n",
    "print(df_yelp.columns)\n",
    "df_yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['zip_code'] = df_yelp['zip_code'].apply(standardize_zipcode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yelp['coordinates'] = list(zip(yelp.latitude, yelp.longitude))\n",
    "# df['x'] = df['x'].round(3)\n",
    "# df['y'] = df['y'].round(3)\n",
    "# df['coordinates'] = list(zip(df.y, df.x))\n",
    "# df = df.drop(columns=['x','y'])\n",
    "# yelp = yelp.drop(columns=['latitude', 'longitude','is_closed'])\n",
    "# yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get type of food\n",
    "food = ['hotdogs', 'sandwiches',\n",
    "       'pizza', 'tradamerican', 'burgers', 'mexican', 'grocery',\n",
    "       'breakfast_brunch', 'coffee', 'chinese', 'italian', 'newamerican',\n",
    "       'chicken_wings', 'delis', 'bars', 'salad', 'seafood', 'bbq', 'bakeries',\n",
    "       'sushi']\n",
    "type_food = []\n",
    "for i, row in df_yelp.iterrows():\n",
    "    type_f = ''\n",
    "    for f in food:  \n",
    "        if row[f] == True:\n",
    "            type_f =f+' '+type_f \n",
    "    if type_f == '':\n",
    "        type_food.append(np.nan)\n",
    "    else:\n",
    "        type_food.append(type_f[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['type_foods'] = type_food\n",
    "df_yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['address1'] = df['address1'].str.upper()\n",
    "# yelp['address1'] = yelp['address1'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['name'] = df['name'].str.upper()\n",
    "# yelp['name'] = yelp['name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip['zip'] = df_zip['zip'].apply(standardize_zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp=df_yelp.rename(columns = {'zip_code':'zip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.rename(columns = {'postalcode':'zip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_yelp = pd.merge(df_yelp,df_zip,  on=['zip'], how='inner')\n",
    "df_income_insp = pd.merge(df1,df_zip,  on=['zip'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_yelp = df_income_yelp.drop(columns=['is_closed','phone'])\n",
    "df_income_yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_insp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_insp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_insp2 = df_income_insp.groupby(['zip'])['score',\n",
    "#        'total_num_inspections', 'total_critical', 'total_non_critical', 'median_family_income_dollars',\n",
    "#        'median_household_income_dollars', 'per_capita_income_dollars',\n",
    "#        'percent_damilies_below_poverty_line', 'percent_snap_benefits',\n",
    "#        'percent_supplemental_security_income', 'percent_nonwhite'].median().reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df_income_insp['zip'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_income_yelp['zip'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp2 = df_income_yelp.groupby(['zip'])['rating', 'review_count'].median().reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.merge(df_income_insp, df_yelp2, on='zip', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=df_1.rename(columns = {'median_household_income_dollars':'med_household_income',\n",
    "                          'median_family_income_dollars':'med_family_income','percent_damilies_below_poverty_line':'percent_families_below_poverty_line',\n",
    "              'per_capita_income_dollars':'per_capita_income', 'total_num_inspections':'total_inspections',\n",
    "                       'rating':'median_yelp_rating','review_count':'median_yelp_review_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the data that we want\n",
    "df = df_1.filter(['zip','score','total_inspections','total_critical', 'med_family_income', 'med_household_income',\n",
    "                                             'median_yelp_rating'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include cells that describe the steps in your data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = pd.plotting.scatter_matrix(df,figsize=(18,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = df.corr()\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = sns.distplot((df['score']), bins=25,kde=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = sns.distplot((df['med_family_income']), bins=25,kde=False)\n",
    "#Family in north carolina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4 = sns.distplot((df['median_yelp_rating']), bins=25,kde=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 =sns.scatterplot(x = df['median_yelp_rating'], y = df['score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4 =sns.scatterplot(x = df['med_family_income'], y = df['score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4 =sns.scatterplot(x = df['med_family_income'], y = df['median_yelp_rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polyfit to fit a 1-degree linear model, predicting health score from yelp review rating\n",
    "a1,b1 = np.polyfit(df['median_yelp_rating'], df['score'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polyfit once more to fit a 1-degree linear model, predicting score from median family income \n",
    "a2,b2 = np.polyfit(df['med_family_income'], df['score'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.arange(min(df['median_yelp_rating']),max(df['median_yelp_rating'])+.1,.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['median_yelp_rating'], df['score'])\n",
    "plt.xlabel('Median Yelp Rating')\n",
    "plt.ylabel('Health Score')\n",
    "plt.plot(x1, a1*x1+b1, c = 'red')\n",
    "f5 = plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=np.arange(min(df['med_family_income']),max(df['med_family_income']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['med_family_income'], df['score'])\n",
    "plt.xlabel('Median Family Income')\n",
    "plt.ylabel('Health Score')\n",
    "plt.plot(x2, a2*x2+b2, c = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_1, predictors_1 = patsy.dmatrices('median_yelp_rating ~ score',df)\n",
    "mod_1 = sm.OLS(outcome_1, predictors_1)\n",
    "res_1 = mod_1.fit()\n",
    "print(res_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_2, predictors_2 = patsy.dmatrices('med_family_income ~ score',df)\n",
    "mod_2 = sm.OLS(outcome_2, predictors_2)\n",
    "res_2 = mod_2.fit()\n",
    "print(res_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = pd.plotting.scatter_matrix(df_1.filter(['rating','total_critical','total_non_critical','score']), figsize = (8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1['facilitytype'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4 = sns.distplot((df_1['score'].loc[df_1['facilitytype']=='Restaurant']), bins=25, kde=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4 = sns.distplot((df_1['rating'].loc[df_1['facilitytype']=='Food Stand']), bins=25, kde=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4 = sns.distplot((df_1['score'].loc[df_1['facilitytype']=='Food Stand']), bins=25, kde=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information that I will be looking for is public hence I will have permission to use it since it is our right to know the grade of each restaurantâ€™s food inspection. There are no privacy concerns regarding the datasets since it is public information. The data could have potential biases by the inspectors, as mention in the reference article (3), inspectors can be biased on how many violations they mark in a restaurant and this can affect the dataset. This could be an ethical problem since an inspector can affect a restaurantâ€™s reputation, therefore we expect that each inspectorâ€™s review is not biased. For the issue of finding inspector biases what can be done is to assign two inspectors to do the same reviews of restaurants and look at both data sets. This will allow us to see if there are any biases in the process of an inspector. In case we see a discrepancy between two inspectors we can ask a third inspector to review the particular restaurant. This will only be able if the Department of Environmental Health of San Diego works closely with us. But this will help prevent collection bias in our process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your discussion information here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
